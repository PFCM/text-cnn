"""
Train a model, either to generate or upsample.
"""
from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import tensorflow as tf

import read
import models


tf.app.flags.DEFINE_integer('rate', 1,
                            'downsampling applied to the data. If it is 1, we '
                            'learn the basic generation model. If > 1 we '
                            'learn a model that takes data downsampled by '
                            '`rate` and upsamples it by a factor of two.')
tf.app.flags.DEFINE_string('logdir', '/tmp/abcdefg',
                           'where to store logfiles and checkpoints.')
tf.app.flags.DEFINE_float('learning_rate', 0.001, 'step size for sgd')
tf.app.flags.DEFINE_integer('batch_size', 32, 'batch size for sgd')
tf.app.flags.DEFINE_integer('sequence_length', 250, 'sequence length. In '
                            'practice it has to be the maximium of this and '
                            'the receptive field of the model')
tf.app.flags.DEFINE_string('data_path', None,
                           'The data. Should be a single text file, '
                           'preprocessed with subword-nmt')
tf.app.flags.DEFINE_string('vocab_path', None,
                           'path to the vocabulary as generated by '
                           'subword-nmt')
tf.app.flags.DEFINE_string('embedding_path', None,
                           'path to the embeddings generated by fastText')


def _downsample_inputs(data, downsample):
    """reduce in time by averaging"""
    data = tf.expand_dims(data, 1)
    data = tf.nn.avg_pool(data,
                          [1, 1, downsample, 1],
                          [1, 1, downsample, 1]
                          'SAME')
    return data


def get_data(data_path,
             vocab_path,
             embedding_path,
             downsampling,
             sequence_length,
             batch_size):
    """get data, embed it and potentially downsample it"""
    with tf.variable_scope('data'):
        vocab = read.load_vocab(vocab_path)
        data_tensor = read.load_dataset(data_path,
                                        vocab,
                                        sequence_length,
                                        batch_size)
    with tf.variable_scope('embeddings'):
        embedding_matrix = read.load_word_embeddings(embedding_path, vocab)
        embedded_data = tf.nn.embedding_lookup(embedding_matrix, data_tensor)
        if downsampling > 1:
            embedded_data = _downsample_inputs(embedded_data, downsample)
    return embedded_data, vocab


def main(_):
    """actually do a things"""
    target_batch, vocab = get_data(FLAGS.data_path,
                                   FLAGS.vocab_path,
                                   FLAGS.embedding_path,
                                   FLAGS.rate//2,
                                   FLAGS.sequence_length,
                                   FLAGS.batch_size)



if __name__ == '__main__':
    tf.app.run()
